\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{multirow}
\usepackage{hyperref}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{ Using News' Title and Body Text Features to Detect Fake News: a Viability Study}


\author{\IEEEauthorblockN{1\textsuperscript{st} Felipe dos Santos Neves}
\IEEEauthorblockA{\textit{DAINF} \\
\textit{UTFPR}\\
Curitiba, Brazil \\
fneves@alunos.utfpr.edu.br}
}

\maketitle

\begin{abstract}

In the past few years there have been a large number of algorithms developed to detect
Fake News. We analyze the viability of using features extracted from both titles and
body from news article for the detection of Fake News using a large dataset and comparing
the results with these available on previous bibliography. We find that features used
to classify a one dataset may not be appropriate to classify from a different one, due
to the differences presented in these datasets. At last we test our model achieving a
classification accuracy of 73.93\%.

\end{abstract}

\begin{IEEEkeywords}
Fake News, Classification, Data Science, Text Features, Unsupervised Learning
\end{IEEEkeywords}

\section{Introduction}

Automatic Fake News detection is a topic of growing interest in the last couple of years,
especially due to the its impact in the 2016 American Elections \cite{allcott_gentzkow_2017}\cite{dey_rafi_parash_arko_chakrabarty_2018},
however it is still very difficult to detect this type of content and stop its spread,
specially in social networks and messaging apps unregulated environments. These situations
alone give enough reason to pursue the definition of reliable approaches to Fake News detection.

In this paper we aim to utilize the results obtained from a previous study done specifically for Fake News
about the 2016 American Election \cite{horne_2017} on a larger and more diverse dataset to determine
their viability in a more general scenario.

\section{Motivation}

The main motivation of this work was to study possible implementations for the development
of Fake News detection algorithms. Many previous studies have already proposed different
algorithms for Fake News detection, but in some cases we found that there is the lack of a
more extensive, reliable and unbiased dataset.

A problem with using smaller datasets is that the results obtained over this dataset may not
be similar to those obtained when using a bigger dataset because these smaller ones more often
than not will present a limited range of variation in their subjects and contents, resulting
in a more clustered distribution of parameters, making it easier for machine learning algorithms
to achieve apparently good results.

It was found that very interesting results were presented in Horne's paper \cite{horne_2017},
however a very small dataset was used, and that is why we chose to use it's proposed features
to analyze a new and bigger dataset.

\section{Previous Results}

We use Horne's results as a baseline for our implementations. In his work he proposes the use of
4 different features from both the title and the body of the news articles. These features are
described in the Table \ref{table:horne_features}.

\begin{table}[htbp]
\caption{Features used in Horne's study}
\begin{center}
\begin{tabular}{ |l|l|l| }
\hline
Section & \multicolumn{2}{ c| }{Feature Sets} \\
\hline
\multirow{4}{*}{Title} & per\_stop & Percentage of stop words\\
    & NN & Number of nouns in the text \\
    & avg\_wlen & Average word length \\
    & FK & Flesch-Kincaid readability index \\ \hline
\multirow{4}{*}{Body} & NN & Number of nouns in the text \\
    & TTR & Token to type ration (lexical richness)\\
    & WC & Number of words in the text \\
    & quote & Number of quotes in the text \\ \hline
\end{tabular}
\label{table:horne_features}
\end{center}
\end{table}

In this paper he also implemented a \textit{SVM} classifier to test the result of these features on his own
proposed dataset. Considering that he only used 4 features, which is a considerably low number for
a text classifier, he achieved a very impressive result with over 78\% accuracy in some tests , as
shown in Table \ref{table:horne_results}.

\begin{table}[htbp]
\caption{Results achieved in Horne's study}
\begin{center}
\begin{tabular}{ |l|l|l| }
\hline
Feature Set & Baseline & Accuracy \\
\hline
Title & 50\% & 78\% \\
\hline
Body & 50\% & 71\% \\
\hline
\end{tabular}
\label{table:horne_results}
\end{center}
\end{table}

For the purpose of comparing the results with a bigger dataset we chose to use a Fake News Corpus
made available by M. Szpakowski \cite{szpakowski_2019} with over 8 million news articles, where around
3 million of them were classified as Fake or Reliable. However due to limitations in time and
computational power available at the time of the development of this project we chose to pick
only 750 entries of both Fake and Reliable news to reduce processing time, while still being able
to achieve meaningful results.

\section{Methods and Implementation}

For the development of this project we utilized the \textit{Python} language, with the aid of some external
libraries as described in the Table \ref{table:python_libs}. \textit{Python} was chosen because it is widely
considered the most accessible language for Data Science and it allows more people to be able to reproduce
the results presented here \cite{cielen_meysman_mohamed_2016}\cite{richard_2018}. The full source
code will also be made available at \url{https://github.com/Feupos/DataScience} for this purpose.
This is a very important topic and will be further discussed in Section \ref{section:discussion}.

\begin{table}[htbp]
\caption{Python Libraries Utilized}
\begin{center}
\begin{tabular}{ |c|c| }
\hline
Library & Purpose \\ \hline
Pandas & File processing and data structures \\ \hline
scikit-learn & Machine learning algorithms \\ \hline
NLTK & Natural language processing tools and dictionaries \\ \hline
SentiStrength & Text polarity evaluation tool \\ \hline
LexicalRichness & Lexical richness evaluation tool (TTR) \\ \hline
Seaborn & Data visualization \\ \hline
\end{tabular}
\label{table:python_libs}
\end{center}
\end{table}

With these tools we developed two scripts, one for extracting the features and another for testing
machine learning algorithms for the classification of Fake News. Two types of classifiers were used:
a linear kernel SVM (Space Vector Machine) classifier and a Random Forest classifier.

We used \textit{scikit-learn}'s \textit{k-fold} cross validation for the performance evaluation of
the classifiers.

Besides the features already mentioned in Table \ref{table:horne_features} an additional set of features
was used for the evaluating the performance of the algorithm, these features are listed on Table
\ref{table:expanded_features} and will be referred as 'expanded feature set' whereas the features from
Table \ref{table:horne_features} will be referred as 'original feature set'. In the same way the dataset
from Horne's work \cite{horne_2017} will be referred as 'original dataset' and the dataset from Szpakowski
\cite{szpakowski_2019} wil be referred as 'expanded dataset'.

\begin{table}[htbp]
\caption{Expanded Feature Set}
\begin{center}
\begin{tabular}{ |l|l|l| }
\hline
\multicolumn{2}{ |c| }{Features} \\
\hline
per\_stop & Percentage of stop words\\
NN & Number of nouns in the text \\
avg\_wlen & Average word length \\
FK & Flesch-Kincaid readability index \\
TTR & Token to type ration (lexical richness)\\
WC & Number of words in the text \\
quote & Number of quotes in the text \\
NNP & Number of proper nouns in the text \\
str\_neg & Average negative polarity of the text \\
str\_pos & Average positive polarity of the text \\
JJR & Number of comparative adjectives in the text \\
JJS & Number of superlative adjectives in the text \\
RBR & Number of comparative adverbs in the text \\
RBS & Number of superlative adverbs in the text \\
\hline
\end{tabular}
\label{table:expanded_features}
\end{center}
\end{table}

\section{Results}

During our tests we used multiple combinations of datasets and feature sets, to be able to highlight the
difference in performance between them. We have also divided our results in 3 categories, one using only
features extracted from the tiles, another with only features extracted from the bodies, and a third one
combining all the extracted features from both titles and bodies.

\subsection{Original Feature Set}

In this test we used both the original feature set and the SVM classifier, to be able to reproduce the results
found in Horne's work \cite{horne_2017}, however we were not able to achieve that, instead the results were
'inverted', meaning that the accuracy using the Title features was achieved using the Body features and vice versa.
This can be observed comparing our results from Table \ref{table:od_of} and the original result from Table \ref{table:horne_results}.

We can also notice how the performance was significantly worse when using the expanded dataset, with the results
on Table \ref{table:ed_of}.

\begin{table}[htbp]
\caption{Original Dataset and Original Features Performance}
\begin{center}
\begin{tabular}{ |l|l|l| }
\hline
Feature Set & Baseline & Accuracy \\
\hline
Title & 50\% & 72.67\% \\
\hline
Body & 50\% & 78.67\% \\
\hline
Combined & 50\% & 80\% \\
\hline
\end{tabular}
\label{table:od_of}
\end{center}
\end{table}

\begin{table}[htbp]
\caption{Expanded Dataset and Original Features Performance}
\begin{center}
\begin{tabular}{ |l|l|l| }
\hline
Feature Set & Baseline & Accuracy \\
\hline
Title & 50\% & 58.13\% \\
\hline
Body & 50\% & 64.27\% \\
\hline
Combined & 50\% & 60.27\% \\
\hline
\end{tabular}
\label{table:ed_of}
\end{center}
\end{table}

\subsection{Expanded Feature Set}

We also tried increasing the number of features in order to try to improve the performance,
specially in the expanded dataset that had a very poor performance with the original features.
In these tests we also switched the classifier for the Random Forest classifier due to the
SVM being to slow to calculate with too many features and entries.

Previous tests had already suggested that the classifier had little impact in the overall performance.
The results are shown in the Table \ref{table:od_ef} and Table \ref{table:od_of}.

\begin{table}[htbp]
\caption{Original Dataset and Expanded Features Performance}
\begin{center}
\begin{tabular}{ |l|l|l| }
\hline
Feature Set & Baseline & Accuracy \\
\hline
Title & 50\% & 71.33\% \\
\hline
Body & 50\% & 82\% \\
\hline
Combined & 50\% & 82\% \\
\hline
\end{tabular}
\label{table:od_ef}
\end{center}
\end{table}

\begin{table}[htbp]
\caption{Expanded Dataset and Expanded Features Performance}
\begin{center}
\begin{tabular}{ |l|l|l| }
\hline
Feature Set & Baseline & Accuracy \\
\hline
Title & 50\% & 61.93\% \\
\hline
Body & 50\% & 73.93\% \\
\hline
Combined & 50\% & 71.8\% \\
\hline
\end{tabular}
\label{table:ed_ef}
\end{center}
\end{table}

\subsection{Compared Results}

For the purpose of being able to better distinguish the results on each step we show
in Figure \ref{fig:results} a comparison between all the results side by side.

\begin{figure}[htbp]
\centerline{
    \includegraphics[width=0.5\textwidth]
    {content/results.png}
    }
\caption{Results comparison}
\label{fig:results}
\end{figure}

\section{Discussion}
\label{section:discussion}

As it is clear to see in Figure \ref{fig:results} it is clear that the performance over
this bigger dataset was significantly worse than in the original paper \cite{horne_2017},
however it is important to understand why we had such difference.

One very obvious reason for the divergence of the results of the classifier for the original
dataset is that the algorithms, libraries and classifiers utilized in our implementation.
This means that even though the author shared the dataset \cite{horne_data_2017}, as they
did not share their implementation and it makes it very hard to reproduce the same results.

In the field of Data Science reproducibility is a very important issue because due to the
problem that finding reliable and unbiased datasets represent we often find the need to
test and validate solutions proposed in other peoples' works. And in cases where either
the source code or the datasets are not available it becomes near impossible to do
\cite{peng_2011}\cite{peng_2015}.

In this case the main problem with the original dataset  is that no only it had only 150
entires, but also that it had only one subject, the 2016 American Elections, and what this
means in terms of the features and classifiers is that the reliable news and fake news will
share many similar characteristics among the items in their groups, an thus making it easier
to achieve a better accuracy on the classifier output. It is easier to separate groups when
their items are very similar inside each group, and share the same differences between items
of the two groups.

As we can see on Figure \ref{fig:titleNN}, some features did not have a  different distribution
over the fake and reliable news, meaning that a classifier would not be able to separate these
classes based on this feature only. Using more features helped overcome this problem but some
features may actually decrease the overall performance.

\begin{figure}[htbp]
\centerline{
    \includegraphics[width=0.5\textwidth]
    {content/titleFK.png}
    }
\caption{FK index distribution over titles}
\label{fig:titleNN}
\end{figure}


This also brings the problem that the features were chosen among the ones with the highest
statistical significance in the original dataset, and this significance may not hold up
in a different dataset such as ours. Another issue is that as the original dataset had the
news manually chosen it is more susceptible to bias, and the fake news chosen were more
explicit, with more distinguishable features compared to reliable news, than the ones in
our expanded dataset.


\section{Conclusion}

The main point that we can drawn from this analysis is that the same feature set may not be
efficient for classifying two different datasets. In this case the characteristics of the two
datasets should be compared to see if they are similar enough. Therefore this approach of
trying to use few but very specific features does not work well in the general case, because
any change in the dataset may reduce the classification performance. To actually improve overall
performance the best approach is to choose a different and wider set of features, and avoid choosing
them based on a specific dataset, this way we can avoid the overfitting of the classification,
where new inputs will have a worse performance due to being just slightly difference from the
initial dataset.

However only using more features is good but may have some drawbacks in regarding the processing
speed that will be increased. Also, if the features set contains features that are not relevant
to the analysis the performance may actually decrease due to containing inputs that cannot be
separated by the classifier, such as what happened with the performance of out classification using
the original and expanded features over the titles of the original dataset, where the performance
dropped from 72.67\% with the original features to 71.33\% when using the expanded features, as shown
on Table \ref{table:od_of} and Table \ref{table:od_ef} respectively. Simply adding more features
will not necessarily cause an improvement in the performance \cite{volkova_shaffer_jang_hodas_2017}.

Another factor that can influence the performance is the choice of the classifier itself.
The SVM is not appropriate for non-normalized data, because each input will influence the others
and therefore the results may be good even though some features are having little impact in the
classification. A more appropriate solution in this cases would be either normalize all the data
or to utilize another type of classifier such as Random Forest, where the inputs do not influence
each other and therefore requires no normalization. Also, when working with larger datasets the
computational power required has a great impact, and the Random Forest algorithm resulted in
significantly lower processing times, to the order of 100 times. But this of course cannot be
generalized as the number of entries and configuration of each classifier can heavily impact
on their speed.




\section*{Acknowledgement}

I would like to thank Prof. Luiz Celso Gomes Junior from UTFPR for the support
provided during the development of this project. It is also important to commend
the work of all the people that collaborate developing free Data Science libraries
and other resources that helps making Data Science more accessible, and without
whom this work would not be possible.

\bibliographystyle{ieeetr}
\bibliography{bibliography}

\end{document}
